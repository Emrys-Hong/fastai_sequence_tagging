{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% reload_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.lm_rnn import *\n",
    "from sebastian.eval import eval_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this cell for only forward direction\"\"\"\n",
    "class LinearDecoder(nn.Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, n_out, n_hid, dropout, tie_encoder=None, bias=False):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropout = LockedDropout(dropout)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, raw_outputs, outputs\n",
    "\n",
    "    \n",
    "class SequentialRNN(nn.Sequential):\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "                \n",
    "                \n",
    "class RNN_Learner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.cross_entropy\n",
    "    def fit(self, *args, **kwargs): return super().fit(*args, **kwargs, seq_first=True)\n",
    "\n",
    "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
    "    def load_encoder(self, name): load_model(self.model[0], self.get_model_path(name))\n",
    "        \n",
    "        \n",
    "class TextModel(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        m = self.model[0]\n",
    "        return [(m.encoder, m.dropouti), *zip(m.rnns, m.dropouths), (self.model[1])]\n",
    "    \n",
    "    \n",
    "def get_rnn_seq_labeler(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, linear_decoder_dp=0.1):\n",
    "    rnn_enc = MultiBatchSeqRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_class, emb_sz, linear_decoder_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this cell for bidir\"\"\"\n",
    "class LinearDecoder_bidir(nn.Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, n_out, n_hid, dropout, tie_encoder=None, bias=False):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropout = LockedDropout(dropout)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        result = decoded.view(-1, decoded.size(1))\n",
    "        return result, raw_outputs, outputs\n",
    "    \n",
    "    \n",
    "##### rewrite sequentialRNN #####\n",
    "'''changed the class it inherits from nn.Sequential to Sequential'''\n",
    "class SequentialRNN_bidir(nn.Module):\n",
    "    def __init__(self, rnn_enc_fw, rnn_enc_bw, linear_decoder, embedding_path, emb_sz, freeze_word2vec=False, wordvec_sz=300):\n",
    "        super().__init__()\n",
    "        self.rnn_enc_fw = rnn_enc_fw\n",
    "        self.rnn_enc_bw = rnn_enc_bw\n",
    "        self.linear_decoder = linear_decoder\n",
    "        self.rnn_lm= nn.LSTM(input_size=emb_sz*2+wordvec_sz*2, hidden_size=emb_sz, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        weights = np.load(embedding_path)\n",
    "        self.embedding = nn.Embedding.from_pretrained(T(weights), freeze=freeze_word2vec)\n",
    "        self.rnn = nn.LSTM(input_size=wordvec_sz, hidden_size=wordvec_sz, num_layers=1, batch_first=True, bidirectional=True)\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "    def forward(self, input):\n",
    "        input_fw = input\n",
    "        lstm_out, (n_h, n_cell) = self.rnn(self.embedding(input))\n",
    "        input_bw = V(np.array([o.cpu().numpy()[::-1] for o in input]))\n",
    "        raw_outputs_fw, outputs_fw = self.rnn_enc_fw(input_fw)\n",
    "        raw_outputs_bw, outputs_bw = self.rnn_enc_bw(input_bw)\n",
    "        bs, sl, _ = outputs_bw[-1].size()\n",
    "        idx = V(torch.LongTensor([i for i in range(sl-1, -1, -1)]))\n",
    "        output_bw = outputs_bw[-1].index_select(1, idx)\n",
    "        outputs_fw_bw = torch.cat([outputs_fw[-1], output_bw], dim=-1)\n",
    "    \n",
    "        ## concat forward raw_outputs & backward raw_outputs together\n",
    "        raw_outputs_bw_ = []\n",
    "        # concat them together\n",
    "        for i in range(3):\n",
    "            bs, sl, _ = raw_outputs_bw[i].size()\n",
    "            idx = V(torch.LongTensor([i for i in range(sl-1, -1, -1)]))\n",
    "            raw_output_bw = raw_outputs_bw[i].index_select(1, idx)\n",
    "            raw_outputs_bw_.append(raw_output_bw)\n",
    "        raw_outputs_fw_bw = [torch.cat([raw_outputs_fw[i], raw_outputs_bw_[i]]) for i in range(3)]\n",
    "        # concat output from lstm_out and rnn_lm\n",
    "        outputs_fw_bw = torch.cat([lstm_out, outputs_fw_bw], dim=-1)\n",
    "        outputs_fw_bw, (n_h, n_cell) = self.rnn_lm(outputs_fw_bw)\n",
    "        out = self.linear_decoder((raw_outputs_fw_bw, outputs_fw_bw.contiguous()))\n",
    "        return out\n",
    "\n",
    "    \n",
    "##### rewrite RNN Learner #####\n",
    "'''rewrite load_encoder to load the encoding modules'''\n",
    "class RNN_Learner_bidir(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.cross_entropy\n",
    "    def fit(self, *args, **kwargs): return super().fit(*args, **kwargs, seq_first=True)\n",
    "\n",
    "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
    "    def load_encoder(self, name_fw, name_bw): \n",
    "        load_model(self.model.rnn_enc_fw, self.get_model_path(name_fw))\n",
    "        load_model(self.model.rnn_enc_bw, self.get_model_path(name_bw))\n",
    "##### end #####\n",
    "\n",
    "\n",
    "##### rewrite textmodel #####\n",
    "'''get layer groups'''\n",
    "class TextModel_bidir(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        m_fw = self.model.rnn_enc_fw\n",
    "        m_bw = self.model.rnn_enc_bw\n",
    "        return [(m_fw.encoder, m_fw.dropouti, m_bw.encoder, m_bw.dropouti), \n",
    "                *zip(m_fw.rnns, m_fw.dropouths, m_bw.rnns, m_bw.dropouths), \n",
    "            (self.model.embedding), (self.model.linear_decoder), (self.model.rnn), (self.model.rnn_lm)]\n",
    "\n",
    "\n",
    "def get_rnn_seq_labeler_bidir(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, linear_decoder_dp=0.1, dir_path='', freeze_word2vec=False):\n",
    "    rnn_enc = MultiBatchSeqRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    rnn_enc_backward = MultiBatchSeqRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    return SequentialRNN_bidir(rnn_enc, rnn_enc_backward, LinearDecoder_bidir(n_class, emb_sz*2, linear_decoder_dp), \n",
    "                               dir_path/'tmp'/'coNLL_embedding.npy', emb_sz, freeze_word2vec=freeze_word2vec, wordvec_sz=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''common functions'''\n",
    "def freeze_all_but(learner, n):\n",
    "    c=learner.get_layer_groups()\n",
    "    for l in c: set_trainable(l, False)\n",
    "    set_trainable(c[n], True)\n",
    "\n",
    "class MultiBatchSeqRNN(RNN_Encoder):\n",
    "    def __init__(self, bptt, max_seq, *args, **kwargs):\n",
    "        self.max_seq,self.bptt = max_seq,bptt\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def concat(self, arrs):\n",
    "        return [torch.cat([l[si] for l in arrs]) for si in range(len(arrs[0]))]\n",
    "\n",
    "    def forward(self, input):\n",
    "        sl,bs = input.size()\n",
    "        for l in self.hidden:\n",
    "            for h in l: h.data.zero_()\n",
    "        raw_outputs, outputs = super().forward(input)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    \n",
    "class SeqDataLoader(DataLoader):\n",
    "    def get_batch(self, indices):\n",
    "        res = self.np_collate([self.dataset[i] for i in indices])\n",
    "        res[1] = np.reshape(res[1], -1)  # reshape the labels to one sequence\n",
    "        return res\n",
    "\n",
    "\n",
    "class TextSeqDataset(Dataset):\n",
    "    def __init__(self, x, y, backwards=False, sos=None, eos=None):\n",
    "        self.x,self.y,self.backwards,self.sos,self.eos = x,y,backwards,sos,eos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]  # we need to get y as array\n",
    "        if self.backwards: x = list(reversed(x))\n",
    "        if self.eos is not None: x = x + [self.eos]\n",
    "        if self.sos is not None: x = [self.sos]+x\n",
    "        return np.array(x),np.array(y)\n",
    "\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq(dir_path, lm_id='', train_file_id='', clas_id=None, bs=64, cl=1, bidir=False, startat=0, unfreeze=True,\n",
    "              lr=0.01, dropmult=1.0, pretrain=True, bpe=False, use_clr=True,\n",
    "              use_regular_schedule=False, use_discriminative=True, last=False, chain_thaw=False,\n",
    "              from_scratch=False, freeze_word2vec=False, n_cycle=3, cycle_len=1, cycle_mult=2, linear_decoder_dp=0.1):\n",
    "    \"\"\"hyperaparameter settings\"\"\"\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "#     bptt, em_sz, nh, nl = 70, 100, 100, 2\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*dropmult\n",
    "#     dps = np.array([0.4,0.5,0.05,0.3,0.7])*dropmult\n",
    "#     dps = np.array([0.5, 0.4, 0.04, 0.3, 0.6])*dropmult\n",
    "    #dps = np.array([0.65,0.48,0.039,0.335,0.34])*dropmult\n",
    "#     dps = np.array([0.6,0.5,0.04,0.3,0.4])*dropmult\n",
    "\n",
    "    print(f'prefix {dir_path}; lm_id {lm_id}; train_file_id {train_file_id}; clas_id {clas_id};'\n",
    "          f' bs {bs}; cl {cl}; bidir {bidir}; '\n",
    "        f'dropmult {dropmult} unfreeze {unfreeze} startat {startat}; pretrain {pretrain}; bpe {bpe}; use_clr {use_clr};'\n",
    "        f' use_regular_schedule {use_regular_schedule}; use_discriminative {use_discriminative}; last {last};'\n",
    "        f' chain_thaw {chain_thaw}; from_scratch {from_scratch}; freeze_word2vec {freeze_word2vec}; bptt {bptt};'\n",
    "          f' em_sz {em_sz}; nh {nh}; nl {nl}; dropouts {dps}; dropmult {dropmult};'\n",
    "         f' linear_decoder_dp {linear_decoder_dp}')\n",
    "    dir_path = Path(dir_path)\n",
    " \n",
    "    lm_file = dir_path/'models'/'lm1_enc'\n",
    "    lm_file_bw = dir_path/'models'/'lm1_enc_backward'\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "    \"\"\"load datasets\"\"\"\n",
    "    trn_sent = np.load(dir_path / 'tmp' / f'trn_ids{train_file_id}.npy')\n",
    "    val_sent = np.load(dir_path / 'tmp' / f'val_ids.npy')\n",
    "    test_sent = np.load(dir_path / 'tmp' / f'test_ids.npy')\n",
    "    trn_lbls = np.load(dir_path / 'tmp' / f'lbl_trn{train_file_id}.npy')\n",
    "    val_lbls = np.load(dir_path / 'tmp' / f'lbl_val.npy')\n",
    "    test_lbls = np.load(dir_path / 'tmp' / f'lbl_test.npy')\n",
    "    id2label = pickle.load(open(dir_path / 'tmp' / 'itol.pkl', 'rb'))\n",
    "    c = len(id2label)\n",
    "\n",
    "    if bpe:\n",
    "        vs=30002\n",
    "    else:\n",
    "        id2token = pickle.load(open(dir_path / 'tmp' / 'itos.pkl', 'rb'))\n",
    "        vs = len(id2token)\n",
    "\n",
    "    print('Train sentences shape:', trn_sent.shape)\n",
    "    print('Train labels shape:', trn_lbls.shape)\n",
    "    print('Token ids:', [id2token[id_] for id_ in trn_sent[0]])\n",
    "    print('Label ids:', [id2label[id_] for id_ in trn_lbls[0]])\n",
    "\n",
    "    trn_ds = TextSeqDataset(trn_sent, trn_lbls)\n",
    "    val_ds = TextSeqDataset(val_sent, val_lbls)\n",
    "    test_ds = TextSeqDataset(test_sent, test_lbls)\n",
    "    trn_samp = SortishSampler(trn_sent, key=lambda x: len(trn_sent[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_sent, key=lambda x: len(val_sent[x]))\n",
    "    test_samp = SortSampler(test_sent, key=lambda x: len(test_sent[x]))\n",
    "    trn_dl = SeqDataLoader(trn_ds, bs//2, transpose=False, num_workers=1, pad_idx=1, sampler=trn_samp)  # TODO why transpose? Should we also transpose the labels?\n",
    "    val_dl = SeqDataLoader(val_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    test_dl = SeqDataLoader(test_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=test_samp)\n",
    "    md = ModelData(dir_path, trn_dl, val_dl, test_dl)\n",
    "\n",
    "    if bidir:\n",
    "        m = get_rnn_seq_labeler_bidir(bptt, 70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "                  layers=[em_sz, 50, c], drops=[dps[4], 0.1],\n",
    "                  dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3], linear_decoder_dp=linear_decoder_dp, \n",
    "                                      freeze_word2vec=freeze_word2vec, dir_path=dir_path, )\n",
    "        learn = RNN_Learner_bidir(md, TextModel_bidir(to_gpu(m)), opt_fn=opt_fn)\n",
    "        learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "        learn.clip=25.\n",
    "        learn.metrics = [accuracy]\n",
    "    else:\n",
    "        m = get_rnn_seq_labeler(bptt, 70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "                  layers=[em_sz, 50, c], drops=[dps[4], 0.1],\n",
    "                  dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3], linear_decoder_dp=linear_decoder_dp)\n",
    "        learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "        learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "        learn.clip=25.\n",
    "        learn.metrics = [accuracy]\n",
    "\n",
    "    \n",
    "\n",
    "    lrm = 2.6\n",
    "    if use_discriminative:\n",
    "#         lrs = np.array([lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "        lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "    else:\n",
    "        lrs = lr\n",
    "    wd = 1e-5\n",
    "    if not from_scratch:\n",
    "        print(f'Loading encoder from {lm_file}...')\n",
    "        if bidir:\n",
    "            learn.load_encoder(lm_file, lm_file_bw)\n",
    "        else:\n",
    "            learn.load_encoder(lm_file)\n",
    "    else:\n",
    "        print('Training classifier from scratch. LM encoder is not loaded.')\n",
    "        use_regular_schedule = True\n",
    "\n",
    "    if (startat<1) and pretrain and not last and not chain_thaw and not from_scratch:\n",
    "        learn.freeze_to(-1)\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "        learn.freeze_to(-2)\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8, 3))\n",
    "        learn.save(f'{PRE}{clas_id}clas_0')\n",
    "    elif startat==1:\n",
    "        learn.load(f'{PRE}{clas_id}clas_0')\n",
    "\n",
    "    if chain_thaw:\n",
    "        lrs = np.array([0.0001, 0.0001, 0.0001, 0.001])\n",
    "        ## Emrys\n",
    "        lrm = 4\n",
    "        # the 4th is too big, and the word embedding and rnn can increase\n",
    "        lrs = np.array([lr/(lrm**5), 2*lr/(lrm**5), lr/(lrm**4), lr/(lrm**4), 5e-4, lr/2, 7e-4, 1e-2])\n",
    "#         lrf = learn.lr_find(lrs) # find the proper learning rate\n",
    "#         learn.sched.plot()\n",
    "        # end\n",
    "        print(f'AWDLSTM learning_rate {lrs[:4]}; embedding_lr {lrs[4]}; linear_decoder_lr {lrs[5]}; rnn_lr {lrs[6]}; lm_lr {lrs[7]}; weight_decay {wd}')\n",
    "        print('Using chain-thaw. Unfreezing all layers one at a time...')\n",
    "        n_layers = len(learn.get_layer_groups())\n",
    "        print('# of layers:', n_layers)\n",
    "        # fine-tune last layer\n",
    "        learn.freeze_to(-1)\n",
    "        print('Fine-tuning layer #7')\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "        n = n_layers-2\n",
    "        # fine-tune all layers up to the second-last one\n",
    "        while n>-1:\n",
    "            print('Fine-tuning layer #%d.' % n)\n",
    "            freeze_all_but(learn, n)\n",
    "            learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                      use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "            n -= 1\n",
    "\n",
    "    if unfreeze:\n",
    "        learn.unfreeze()\n",
    "    else:\n",
    "        learn.freeze_to(-3)\n",
    "\n",
    "    if last:\n",
    "        print('Fine-tuning only the last layer...')\n",
    "        learn.freeze_to(-1)\n",
    "\n",
    "    if use_regular_schedule:\n",
    "        print('Using regular schedule. Setting use_clr=None, n_cycles=cl, cycle_len=None.')\n",
    "        use_clr = None\n",
    "        n_cycle = n_cycle\n",
    "        cycle_len = None\n",
    "    else:\n",
    "        n_cycle = n_cycle\n",
    "    print(f'n_cycle {n_cycle}; cycle_len {cycle_len}; cycle_mult {cycle_mult}; use_clr {use_clr}')\n",
    "    learn.fit(lrs, n_cycle, wds=wd, cycle_len=cycle_len, cycle_mult=cycle_mult, use_clr=(8,8) if use_clr else None) # previously cycle_len=cl\n",
    "    print('Plotting lrs...')\n",
    "    learn.sched.plot_lr()\n",
    "    clas_id = clas_id if clas_id is not None else lm_id\n",
    "    bidir = 'bidir' if bidir else 'forward'\n",
    "    learn.save(f'{clas_id}clas_1{bidir}')\n",
    "\n",
    "    eval_ner(learn, id2label, is_test=False)\n",
    "    eval_ner(learn, id2label, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix /fs-object-detection/paperspace/fastai/courses/coNLL/data/nlp_seq/ner/; lm_id ; train_file_id ; clas_id None; bs 64; cl 1; bidir True; dropmult 1 unfreeze True startat 0; pretrain True; bpe False; use_clr False; use_regular_schedule False; use_discriminative True; last False; chain_thaw True; from_scratch False; freeze_word2vec False; bptt 70; em_sz 400; nh 1150; nl 3; dropouts [0.4  0.5  0.05 0.3  0.4 ]; dropmult 1; linear_decoder_dp 0.2\n",
      "Train sentences shape: (14988,)\n",
      "Train labels shape: (14988,)\n",
      "Token ids: ['xbos', '-docstart-']\n",
      "Label ids: ['_bos_', 'O']\n",
      "Loading encoder from /fs-object-detection/paperspace/fastai/courses/coNLL/data/nlp_seq/ner/models/lm1_enc...\n",
      "AWDLSTM learning_rate [0.00001 0.00002 0.00004 0.00004]; embedding_lr 0.0005; linear_decoder_lr 0.005; rnn_lr 0.0007; lm_lr 0.01; weight_decay 1e-05\n",
      "Using chain-thaw. Unfreezing all layers one at a time...\n",
      "# of layers: 8\n",
      "Fine-tuning layer #7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93741108e674636a47a7f6ca46c18bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.159474   0.186139   0.948293  \n",
      "\n",
      "Fine-tuning layer #6.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7e60369a684ecb924a25497c250fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.122185   0.153179   0.959319  \n",
      "\n",
      "Fine-tuning layer #5.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7cc14d072f411a81ef87f64b5a4fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.098153   0.131593   0.960315  \n",
      "\n",
      "Fine-tuning layer #4.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2374ceed7014d008961059438b286c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.093267   0.129416   0.961494  \n",
      "\n",
      "Fine-tuning layer #3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb534368138846fda4fd5b13a7ddecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.102729   0.13574    0.961593  \n",
      "\n",
      "Fine-tuning layer #2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820ea159b7447adb5d4afd295b53274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.093768   0.136768   0.961793  \n",
      "\n",
      "Fine-tuning layer #1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e6836351de4ee596359c36a2063c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.100634   0.13684    0.96171   \n",
      "\n",
      "Fine-tuning layer #0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c24cda9b29481cb0465fd7bc1e9383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.091858   0.136862   0.96171   \n",
      "\n",
      "n_cycle 4; cycle_len 1; cycle_mult 2; use_clr False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771c25ab715540ff90896e31b64ececa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      0.087901   0.120198   0.965296  \n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "train_seq('/fs-object-detection/paperspace/fastai/courses/coNLL/data/nlp_seq/ner/', lm_id='', train_file_id='', clas_id=None,\n",
    "          bs=64, cl=1, bidir=True, startat=0, unfreeze=True,\n",
    "              lr=0.01, dropmult=1, pretrain=True, bpe=False, use_clr=False,\n",
    "              use_regular_schedule=False, use_discriminative=True, last=False, chain_thaw=True,\n",
    "              from_scratch=False, freeze_word2vec=False, n_cycle=4, cycle_len=1, cycle_mult=2, linear_decoder_dp=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
