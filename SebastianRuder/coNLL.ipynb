{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import fire\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "BOS_LABEL = '_bos_'\n",
    "PAD = '_pad_'\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "\n",
    "def read_file(filepath):\n",
    "    assert os.path.exists(filepath)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        sentence = [BOS]\n",
    "        sentence_labels = [BOS_LABEL]\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                sentences.append(sentence)\n",
    "                labels.append(sentence_labels)\n",
    "                sentence = [BOS]  # use xbos as the start of sentence token\n",
    "                sentence_labels = [BOS_LABEL]\n",
    "            else:\n",
    "                sentence.append(line.split()[0].lower())\n",
    "                # label is generally in the last column\n",
    "                sentence_labels.append(line.split()[-1])\n",
    "        if sentence:  # some files, e.g. NER end on an empty line\n",
    "            sentences.append(sentence)\n",
    "            labels.append(sentence_labels)\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def create_toks(prefix, max_vocab=30000, min_freq=1):\n",
    "    PATH = f'data/nlp_seq/{prefix}/'\n",
    "\n",
    "    names = {}\n",
    "    if prefix == 'ner':\n",
    "        names['train'] = 'train.txt'\n",
    "        names['val'] = 'valid.txt'\n",
    "        names['test'] = 'test.txt'\n",
    "    else:\n",
    "        raise ValueError(f'Filenames for {prefix} have to be added first.')\n",
    "    paths = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        paths[split] = f'{PATH}{names[split]}'\n",
    "\n",
    "    print(f'prefix {prefix} max_vocab {max_vocab} min_freq {min_freq}')\n",
    "\n",
    "    os.makedirs(f'{PATH}tmp', exist_ok=True)\n",
    "    trn_tok, trn_labels = read_file(paths['train'])\n",
    "    val_tok, val_labels = read_file(paths['val'])\n",
    "    test_tok, test_labels = read_file(paths['test'])\n",
    "\n",
    "    for trn_t, trn_l in zip(trn_tok[:5], trn_labels[:5]):\n",
    "        print('Sentence:', trn_t, 'labels:', trn_l)\n",
    "\n",
    "    print(f'# of train: {len(trn_tok)}, # of val: {len(val_tok)},'\n",
    "          f'# of test: {len(test_tok)}')\n",
    "\n",
    "    freq = Counter(p for o in trn_tok for p in o)\n",
    "    print(freq.most_common(25))\n",
    "    itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "    itos.insert(0, PAD)\n",
    "    itos.insert(0, '_unk_')\n",
    "    stoi = collections.defaultdict(lambda: 0,\n",
    "                                   {v: k for k, v in enumerate(itos)})\n",
    "    print(len(itos))\n",
    "\n",
    "    trn_ids = np.array([[stoi[o] for o in p] for p in trn_tok])\n",
    "    val_ids = np.array([[stoi[o] for o in p] for p in val_tok])\n",
    "    test_ids = np.array([[stoi[o] for o in p] for p in test_tok])\n",
    "\n",
    "    # map the labels to ids\n",
    "    freq = Counter(p for o in trn_labels for p in o)\n",
    "    print(freq)\n",
    "    itol = [l for l, c in freq.most_common()]\n",
    "    itol.insert(1, PAD)  # insert padding label at index 1\n",
    "    print(itol)\n",
    "    ltoi = {l: i for i, l in enumerate(itol)}\n",
    "    trn_lbl_ids = np.array([[ltoi[o] for o in p] for p in trn_labels])\n",
    "    val_lbl_ids = np.array([[ltoi[o] for o in p] for p in val_labels])\n",
    "    test_lbl_ids = np.array([[ltoi[o] for o in p] for p in test_labels])\n",
    "\n",
    "    ids_joined = np.array([[stoi[o] for o in p] for p in trn_tok + val_tok + test_tok])\n",
    "    val_ids_joined = ids_joined[int(len(ids_joined)*0.9):]\n",
    "    ids_joined = ids_joined[:int(len(ids_joined)*0.9)]\n",
    "\n",
    "    np.save(f'{PATH}tmp/trn_ids.npy', trn_ids)\n",
    "    np.save(f'{PATH}tmp/val_ids.npy', val_ids)\n",
    "    np.save(f'{PATH}tmp/test_ids.npy', test_ids)\n",
    "    np.save(f'{PATH}tmp/lbl_trn.npy', trn_lbl_ids)\n",
    "    np.save(f'{PATH}tmp/lbl_val.npy', val_lbl_ids)\n",
    "    np.save(f'{PATH}tmp/lbl_test.npy', test_lbl_ids)\n",
    "    pickle.dump(itos, open(f'{PATH}tmp/itos.pkl', 'wb'))\n",
    "    pickle.dump(itol, open(f'{PATH}tmp/itol.pkl', 'wb'))\n",
    "    np.save(f'{PATH}tmp/trn_lm_ids.npy', ids_joined)\n",
    "    np.save(f'{PATH}tmp/val_lm_ids.npy', val_ids_joined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix ner max_vocab 30000 min_freq 1\n",
      "Sentence: ['xbos', '-docstart-'] labels: ['_bos_', 'O']\n",
      "Sentence: ['xbos', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'] labels: ['_bos_', 'I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n",
      "Sentence: ['xbos', 'peter', 'blackburn'] labels: ['_bos_', 'I-PER', 'I-PER']\n",
      "Sentence: ['xbos', 'brussels', '1996-08-22'] labels: ['_bos_', 'I-LOC', 'O']\n",
      "Sentence: ['xbos', 'the', 'european', 'commission', 'said', 'on', 'thursday', 'it', 'disagreed', 'with', 'german', 'advice', 'to', 'consumers', 'to', 'shun', 'british', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.'] labels: ['_bos_', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "# of train: 14988, # of val: 3468,# of test: 3686\n",
      "[('xbos', 14988), ('the', 8390), ('.', 7374), (',', 7290), ('of', 3815), ('in', 3621), ('to', 3424), ('a', 3199), ('and', 2872), ('(', 2861), (')', 2861), ('\"', 2178), ('on', 2092), ('said', 1849), (\"'s\", 1566), ('for', 1465), ('1', 1421), ('-', 1243), ('at', 1146), ('was', 1095), ('2', 973), ('-docstart-', 946), ('0', 945), ('3', 932), ('with', 867)]\n",
      "10953\n",
      "Counter({'O': 170524, '_bos_': 14988, 'I-PER': 11128, 'I-ORG': 10001, 'I-LOC': 8286, 'I-MISC': 4556, 'B-MISC': 37, 'B-ORG': 24, 'B-LOC': 11})\n",
      "['O', '_pad_', '_bos_', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "create_toks('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% reload_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.init' has no attribute 'normal_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e10b9d9a6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_rnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_ner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fs-object-detection/paperspace/fastai/courses/coNLL/eval.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_toks_conll\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBOS_LABEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpanBasedF1Measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/training/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpeak_memory_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_memory_mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/dataset_readers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# pylint: disable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconll2003\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConll2003DatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoreference_resolution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConllCorefReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWinobiasReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/dataset_readers/conll2003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_readers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miob1_to_bioul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequenceLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/instance.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/fields/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_field\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_field\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknowledge_graph_field\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKnowledgeGraphField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_field\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_field\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/data/fields/knowledge_graph_field.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknowledge_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKnowledgeGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInitializerApplicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegularizerApplicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/allennlp/nn/initializers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# There are no classes to decorate, so we hack these into Registrable._registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m Registrable._registry[Initializer] = {  # pylint: disable=protected-access\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_initializer_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_initializer_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;34m\"orthogonal\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_initializer_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morthogonal_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.init' has no attribute 'normal_'"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "from fastai.text import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "from eval import eval_ner\n",
    "\n",
    "\n",
    "def freeze_all_but(learner, n):\n",
    "    c=learner.get_layer_groups()\n",
    "    for l in c: set_trainable(l, False)\n",
    "    set_trainable(c[n], True)\n",
    "\n",
    "\n",
    "def get_rnn_seq_labeler(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    rnn_enc = MultiBatchSeqRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    # return SequentialRNN(rnn_enc, LinearBlocks(layers, drops))\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_class, emb_sz, 0.1))\n",
    "\n",
    "\n",
    "class MultiBatchSeqRNN(RNN_Encoder):\n",
    "    def __init__(self, bptt, max_seq, *args, **kwargs):\n",
    "        self.max_seq,self.bptt = max_seq,bptt\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def concat(self, arrs):\n",
    "        return [torch.cat([l[si] for l in arrs]) for si in range(len(arrs[0]))]\n",
    "\n",
    "    def forward(self, input):\n",
    "        sl,bs = input.size()\n",
    "        for l in self.hidden:\n",
    "            for h in l: h.data.zero_()\n",
    "        # raw_outputs, outputs = [],[]\n",
    "        raw_outputs, outputs = super().forward(input)\n",
    "        # for i in range(0, sl, self.bptt):\n",
    "        #     r, o = super().forward(input[i: min(i+self.bptt, sl)])\n",
    "        #     if i>(sl-self.max_seq):\n",
    "        #         raw_outputs.append(r)\n",
    "        #         outputs.append(o)\n",
    "        # return self.concat(raw_outputs), self.concat(outputs)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "\n",
    "class SeqDataLoader(DataLoader):\n",
    "    def get_batch(self, indices):\n",
    "        res = self.np_collate([self.dataset[i] for i in indices])\n",
    "        # res = self.np_collate([self.dataset[i] for i in indices], self.pad_idx)\n",
    "        # if not self.transpose: return res\n",
    "        # res[0] = res[0].T\n",
    "        # print('First seq:', res[0][0])\n",
    "        # print('First labels:', res[1][0])\n",
    "        res[1] = np.reshape(res[1], -1)  # reshape the labels to one sequence\n",
    "        return res\n",
    "\n",
    "\n",
    "class TextSeqDataset(Dataset):\n",
    "    def __init__(self, x, y, backwards=False, sos=None, eos=None):\n",
    "        self.x,self.y,self.backwards,self.sos,self.eos = x,y,backwards,sos,eos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]  # we need to get y as array\n",
    "        if self.backwards: x = list(reversed(x))\n",
    "        if self.eos is not None: x = x + [self.eos]\n",
    "        if self.sos is not None: x = [self.sos]+x\n",
    "        return np.array(x),np.array(y)\n",
    "\n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "\n",
    "def train_seq(dir_path, cuda_id, lm_id='', clas_id=None, bs=64, cl=1, backwards=False, startat=0, unfreeze=True,\n",
    "              lr=0.01, dropmult=1.0, pretrain=True, bpe=False, use_clr=True,\n",
    "              use_regular_schedule=False, use_discriminative=True, last=False, chain_thaw=False,\n",
    "              from_scratch=False, train_file_id=''):\n",
    "    print(f'prefix {dir_path}; cuda_id {cuda_id}; lm_id {lm_id}; clas_id {clas_id}; bs {bs}; cl {cl}; backwards {backwards}; '\n",
    "        f'dropmult {dropmult} unfreeze {unfreeze} startat {startat}; pretrain {pretrain}; bpe {bpe}; use_clr {use_clr};'\n",
    "        f'use_regular_schedule {use_regular_schedule}; use_discriminative {use_discriminative}; last {last};'\n",
    "        f'chain_thaw {chain_thaw}; from_scratch {from_scratch}; train_file_id {train_file_id}')\n",
    "\n",
    "    if not hasattr(torch._C, '_cuda_setDevice'):\n",
    "        print('CUDA not available. Setting device=-1.')\n",
    "        cuda_id = -1\n",
    "    torch.cuda.set_device(cuda_id)\n",
    "    PRE = 'bwd_' if backwards else 'fwd_'\n",
    "    PRE = 'bpe_' + PRE if bpe else PRE\n",
    "    IDS = 'bpe' if bpe else 'ids'\n",
    "    dir_path = Path(dir_path)\n",
    "    train_file_id = train_file_id if train_file_id == '' else f'_{train_file_id}'\n",
    "    lm_id = lm_id if lm_id == '' else f'{lm_id}_'\n",
    "    clas_id = lm_id if clas_id is None else clas_id\n",
    "    clas_id = clas_id if clas_id == '' else f'{clas_id}_'\n",
    "    lm_file = '/fs-object-detection/paperspace/fastai/courses/coNLL/data/models/lm1_enc' # there is changed by Emrys\n",
    "    \n",
    "    lm_path = dir_path / 'models' / f'{lm_file}.h5'\n",
    "    if not from_scratch:\n",
    "        assert lm_path.exists(), f'Error: {lm_path} does not exist.'\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "#     bptt, em_sz, nh, nl = 70, 100, 100, 2\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "    if backwards:\n",
    "        trn_sent = np.load(dir_path / 'tmp' / f'trn_{IDS}{train_file_id}_bwd.npy')\n",
    "        val_sent = np.load(dir_path / 'tmp' / f'val_{IDS}_bwd.npy')\n",
    "        test_sent = np.load(dir_path / 'tmp' / f'test_{IDS}_bwd.npy')\n",
    "    else:\n",
    "        trn_sent = np.load(dir_path / 'tmp' / f'trn_{IDS}{train_file_id}.npy')\n",
    "        val_sent = np.load(dir_path / 'tmp' / f'val_{IDS}.npy')\n",
    "        test_sent = np.load(dir_path / 'tmp' / f'test_{IDS}.npy')\n",
    "\n",
    "    trn_lbls = np.load(dir_path / 'tmp' / f'lbl_trn{train_file_id}.npy')\n",
    "    val_lbls = np.load(dir_path / 'tmp' / f'lbl_val.npy')\n",
    "    test_lbls = np.load(dir_path / 'tmp' / f'lbl_test.npy')\n",
    "    id2label = pickle.load(open(dir_path / 'tmp' / 'itol.pkl', 'rb'))\n",
    "    c = len(id2label)\n",
    "\n",
    "    if bpe:\n",
    "        vs=30002\n",
    "    else:\n",
    "        id2token = pickle.load(open(dir_path / 'tmp' / 'itos.pkl', 'rb'))\n",
    "        vs = len(id2token)\n",
    "\n",
    "    print('Train sentences shape:', trn_sent.shape)\n",
    "    print('Train labels shape:', trn_lbls.shape)\n",
    "    print('Token ids:', [id2token[id_] for id_ in trn_sent[0]])\n",
    "    print('Label ids:', [id2label[id_] for id_ in trn_lbls[0]])\n",
    "\n",
    "    trn_ds = TextSeqDataset(trn_sent, trn_lbls)\n",
    "    val_ds = TextSeqDataset(val_sent, val_lbls)\n",
    "    test_ds = TextSeqDataset(test_sent, test_lbls)\n",
    "    trn_samp = SortishSampler(trn_sent, key=lambda x: len(trn_sent[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_sent, key=lambda x: len(val_sent[x]))\n",
    "    test_samp = SortSampler(test_sent, key=lambda x: len(test_sent[x]))\n",
    "    trn_dl = SeqDataLoader(trn_ds, bs//2, transpose=False, num_workers=1, pad_idx=1, sampler=trn_samp)  # TODO why transpose? Should we also transpose the labels?\n",
    "    val_dl = SeqDataLoader(val_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    test_dl = SeqDataLoader(test_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=test_samp)\n",
    "    md = ModelData(dir_path, trn_dl, val_dl, test_dl)\n",
    "\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*dropmult\n",
    "    #dps = np.array([0.5, 0.4, 0.04, 0.3, 0.6])*dropmult\n",
    "    #dps = np.array([0.65,0.48,0.039,0.335,0.34])*dropmult\n",
    "    #dps = np.array([0.6,0.5,0.04,0.3,0.4])*dropmult\n",
    "\n",
    "    m = get_rnn_seq_labeler(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "    lrm = 2.6\n",
    "    if use_discriminative:\n",
    "#         lrs = np.array([lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "        ## Emrys\n",
    "        lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "        ## end\n",
    "    else:\n",
    "        lrs = lr\n",
    "    wd = 1e-6\n",
    "    if not from_scratch:\n",
    "        print(f'Loading encoder from {lm_file}...')\n",
    "        learn.load_encoder(lm_file)\n",
    "    else:\n",
    "        print('Training classifier from scratch. LM encoder is not loaded.')\n",
    "        use_regular_schedule = True\n",
    "\n",
    "    if (startat<1) and pretrain and not last and not chain_thaw and not from_scratch:\n",
    "        learn.freeze_to(-1)\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "        learn.freeze_to(-2)\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8, 3))\n",
    "        learn.save(f'{PRE}{clas_id}clas_0')\n",
    "    elif startat==1:\n",
    "        learn.load(f'{PRE}{clas_id}clas_0')\n",
    "\n",
    "    if chain_thaw:\n",
    "        lrs = np.array([0.0001, 0.0001, 0.0001, 0.001])\n",
    "        ## Emrys\n",
    "        lrm = 2.6\n",
    "        lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "        # end\n",
    "        print('Using chain-thaw. Unfreezing all layers one at a time...')\n",
    "        n_layers = len(learn.get_layer_groups())\n",
    "        print('# of layers:', n_layers)\n",
    "        # fine-tune last layer\n",
    "        learn.freeze_to(-1)\n",
    "        print('Fine-tuning last layer...')\n",
    "        learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                  use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "        n = 0\n",
    "        # fine-tune all layers up to the second-last one\n",
    "        while n < n_layers-1:\n",
    "            print('Fine-tuning layer #%d.' % n)\n",
    "            freeze_all_but(learn, n)\n",
    "            learn.fit(lrs, 1, wds=wd, cycle_len=None if use_regular_schedule else 1,\n",
    "                      use_clr=None if use_regular_schedule or not use_clr else (8,3))\n",
    "            n += 1\n",
    "\n",
    "    if unfreeze:\n",
    "        learn.unfreeze()\n",
    "    else:\n",
    "        learn.freeze_to(-3)\n",
    "\n",
    "    if last:\n",
    "        print('Fine-tuning only the last layer...')\n",
    "        learn.freeze_to(-1)\n",
    "\n",
    "    if use_regular_schedule:\n",
    "        print('Using regular schedule. Setting use_clr=None, n_cycles=cl, cycle_len=None.')\n",
    "        use_clr = None\n",
    "        n_cycles = cl\n",
    "        cl = None\n",
    "    else:\n",
    "        n_cycles = 1\n",
    "    learn.fit(lrs, n_cycles, wds=wd, cycle_len=cl, use_clr=(8,8) if use_clr else None)\n",
    "    print('Plotting lrs...')\n",
    "    learn.sched.plot_lr()\n",
    "    learn.save(f'{PRE}{clas_id}clas_1')\n",
    "\n",
    "    eval_ner(learn, id2label, is_test=False)\n",
    "    eval_ner(learn, id2label, is_test=True)\n",
    "\n",
    "# if __name__ == '__main__': fire.Fire(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq('/fs-object-detection/paperspace/fastai/courses/coNLL/data/nlp_seq/ner/', 0, lm_id='', clas_id=None, bs=64, cl=1, backwards=False, startat=0, unfreeze=True,\n",
    "              lr=0.01, dropmult=1.0, pretrain=True, bpe=False, use_clr=True,\n",
    "              use_regular_schedule=False, use_discriminative=True, last=False, chain_thaw=True,\n",
    "              from_scratch=False, train_file_id='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences shape: (14988,)\n",
      "Train labels shape: (14988,)\n",
      "Token ids: ['xbos', '-docstart-']\n",
      "Label ids: ['_bos_', 'O']\n"
     ]
    }
   ],
   "source": [
    "class SeqDataLoader(DataLoader):\n",
    "    def get_batch(self, indices):\n",
    "        res = self.np_collate([self.dataset[i] for i in indices])\n",
    "        # res = self.np_collate([self.dataset[i] for i in indices], self.pad_idx)\n",
    "        # if not self.transpose: return res\n",
    "        # res[0] = res[0].T\n",
    "        # print('First seq:', res[0][0])\n",
    "        # print('First labels:', res[1][0])\n",
    "        res[1] = np.reshape(res[1], -1)  # reshape the labels to one sequence\n",
    "        return res\n",
    "\n",
    "\n",
    "class TextSeqDataset(Dataset):\n",
    "    def __init__(self, x, y, backwards=False, sos=None, eos=None):\n",
    "        self.x,self.y,self.backwards,self.sos,self.eos = x,y,backwards,sos,eos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]  # we need to get y as array\n",
    "        if self.backwards: x = list(reversed(x))\n",
    "        if self.eos is not None: x = x + [self.eos]\n",
    "        if self.sos is not None: x = [self.sos]+x\n",
    "        return np.array(x),np.array(y)\n",
    "\n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "dir_path = Path('/fs-object-detection/paperspace/fastai/courses/coNLL/data/nlp_seq/ner/')\n",
    "bs=64 # batch_size = 2\n",
    "trn_sent = np.load(dir_path / 'tmp' / f'trn_ids.npy')\n",
    "val_sent = np.load(dir_path / 'tmp' / f'val_ids.npy')\n",
    "test_sent = np.load(dir_path / 'tmp' / f'test_ids.npy')\n",
    "\n",
    "trn_lbls = np.load(dir_path / 'tmp' / f'lbl_trn.npy')\n",
    "val_lbls = np.load(dir_path / 'tmp' / f'lbl_val.npy')\n",
    "test_lbls = np.load(dir_path / 'tmp' / f'lbl_test.npy')\n",
    "id2label = pickle.load(open(dir_path / 'tmp' / 'itol.pkl', 'rb'))\n",
    "c = len(id2label) + 2 # for start tag and end tag\n",
    "\n",
    "\n",
    "\n",
    "id2token = pickle.load(open(dir_path / 'tmp' / 'itos.pkl', 'rb'))\n",
    "vs = len(id2token)\n",
    "\n",
    "print('Train sentences shape:', trn_sent.shape)\n",
    "print('Train labels shape:', trn_lbls.shape)\n",
    "print('Token ids:', [id2token[id_] for id_ in trn_sent[0]])\n",
    "print('Label ids:', [id2label[id_] for id_ in trn_lbls[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_ds = TextSeqDataset(trn_sent, trn_lbls)\n",
    "val_ds = TextSeqDataset(val_sent, val_lbls)\n",
    "test_ds = TextSeqDataset(test_sent, test_lbls)\n",
    "trn_samp = SortishSampler(trn_sent, key=lambda x: len(trn_sent[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_sent, key=lambda x: len(val_sent[x]))\n",
    "test_samp = SortSampler(test_sent, key=lambda x: len(test_sent[x]))\n",
    "trn_dl = SeqDataLoader(trn_ds, bs//2, transpose=False, num_workers=1, pad_idx=1, sampler=trn_samp)  # TODO why transpose? Should we also transpose the labels?\n",
    "val_dl = SeqDataLoader(val_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "test_dl = SeqDataLoader(test_ds, bs, transpose=False, num_workers=1, pad_idx=1, sampler=test_samp)\n",
    "md = ModelData('/fs-object-detection/paperspace/fastai/courses/coNLL/data', trn_dl, val_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 0\n",
       " 0\n",
       "⋮ \n",
       " 0\n",
       " 5\n",
       " 0\n",
       "[torch.cuda.LongTensor of size 3648 (GPU 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trn_dl))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.7632\n",
       " 1.1393\n",
       " 0.8263\n",
       "[torch.cuda.FloatTensor of size 3 (GPU 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V(x)[(V(x) > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,j in md.trn_dl.dataset:\n",
    "    if len(i)!=len(j):\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3648]) torch.Size([3648])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([96]) torch.Size([96])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([1408]) torch.Size([1408])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([1376]) torch.Size([1376])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([640]) torch.Size([640])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1152]) torch.Size([1152])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([1344]) torch.Size([1344])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([1152]) torch.Size([1152])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([1248]) torch.Size([1248])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1152]) torch.Size([1152])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1248]) torch.Size([1248])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([96]) torch.Size([96])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([1248]) torch.Size([1248])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([1440]) torch.Size([1440])\n",
      "torch.Size([1344]) torch.Size([1344])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([1696]) torch.Size([1696])\n",
      "torch.Size([1472]) torch.Size([1472])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([1344]) torch.Size([1344])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([2528]) torch.Size([2528])\n",
      "torch.Size([1504]) torch.Size([1504])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([1344]) torch.Size([1344])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([640]) torch.Size([640])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([2016]) torch.Size([2016])\n",
      "torch.Size([1472]) torch.Size([1472])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1408]) torch.Size([1408])\n",
      "torch.Size([1344]) torch.Size([1344])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([2016]) torch.Size([2016])\n",
      "torch.Size([1408]) torch.Size([1408])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([1856]) torch.Size([1856])\n",
      "torch.Size([1440]) torch.Size([1440])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([1248]) torch.Size([1248])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([960]) torch.Size([960])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([544]) torch.Size([544])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1376]) torch.Size([1376])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([1920]) torch.Size([1920])\n",
      "torch.Size([1376]) torch.Size([1376])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([576]) torch.Size([576])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([1376]) torch.Size([1376])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([1280]) torch.Size([1280])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([736]) torch.Size([736])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([960]) torch.Size([960])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800]) torch.Size([800])\n",
      "torch.Size([672]) torch.Size([672])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([96]) torch.Size([96])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([1056]) torch.Size([1056])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([1792]) torch.Size([1792])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([2176]) torch.Size([2176])\n",
      "torch.Size([1440]) torch.Size([1440])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([352]) torch.Size([352])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1152]) torch.Size([1152])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([640]) torch.Size([640])\n",
      "torch.Size([608]) torch.Size([608])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([1312]) torch.Size([1312])\n",
      "torch.Size([1216]) torch.Size([1216])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([288]) torch.Size([288])\n",
      "torch.Size([896]) torch.Size([896])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([224]) torch.Size([224])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([160]) torch.Size([160])\n",
      "torch.Size([96]) torch.Size([96])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1120]) torch.Size([1120])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1888]) torch.Size([1888])\n",
      "torch.Size([1472]) torch.Size([1472])\n",
      "torch.Size([416]) torch.Size([416])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([928]) torch.Size([928])\n",
      "torch.Size([640]) torch.Size([640])\n",
      "torch.Size([768]) torch.Size([768])\n",
      "torch.Size([1184]) torch.Size([1184])\n",
      "torch.Size([1088]) torch.Size([1088])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([320]) torch.Size([320])\n",
      "torch.Size([864]) torch.Size([864])\n",
      "torch.Size([832]) torch.Size([832])\n",
      "torch.Size([512]) torch.Size([512])\n",
      "torch.Size([480]) torch.Size([480])\n",
      "torch.Size([256]) torch.Size([256])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([1024]) torch.Size([1024])\n",
      "torch.Size([992]) torch.Size([992])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([448]) torch.Size([448])\n",
      "torch.Size([704]) torch.Size([704])\n",
      "torch.Size([252]) torch.Size([252])\n"
     ]
    }
   ],
   "source": [
    "for i,j in iter(md.trn_dl):\n",
    "    print(i.view(-1).size(), j.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?? DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,2,3,4).size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_crf_scrf_label():\n",
    "    SCRF_l_map = {}\n",
    "    SCRF_l_map['PER'] = 0\n",
    "    SCRF_l_map['LOC'] = 1\n",
    "    SCRF_l_map['ORG'] = 2\n",
    "    SCRF_l_map['MISC'] = 3\n",
    "    CRF_l_map = {}\n",
    "    for pre in ['S-', 'B-', 'I-', 'E-']:\n",
    "        for suf in SCRF_l_map.keys():\n",
    "            CRF_l_map[pre + suf] = len(CRF_l_map)\n",
    "    SCRF_l_map['<START>'] = 4\n",
    "    SCRF_l_map['<STOP>'] = 5\n",
    "    SCRF_l_map['O'] = 6\n",
    "    CRF_l_map['<start>'] = len(CRF_l_map)\n",
    "    CRF_l_map['<pad>'] = len(CRF_l_map)\n",
    "    CRF_l_map['O'] = len(CRF_l_map)\n",
    "\n",
    "    return CRF_l_map, SCRF_l_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crf_table, scrf_table = get_crf_scrf_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 17,\n",
       " '<start>': 16,\n",
       " 'B-LOC': 5,\n",
       " 'B-MISC': 7,\n",
       " 'B-ORG': 6,\n",
       " 'B-PER': 4,\n",
       " 'E-LOC': 13,\n",
       " 'E-MISC': 15,\n",
       " 'E-ORG': 14,\n",
       " 'E-PER': 12,\n",
       " 'I-LOC': 9,\n",
       " 'I-MISC': 11,\n",
       " 'I-ORG': 10,\n",
       " 'I-PER': 8,\n",
       " 'O': 18,\n",
       " 'S-LOC': 1,\n",
       " 'S-MISC': 3,\n",
       " 'S-ORG': 2,\n",
       " 'S-PER': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<START>': 4, '<STOP>': 5, 'LOC': 1, 'MISC': 3, 'O': 6, 'ORG': 2, 'PER': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(10).reshape(2,5) > np.ones((2,5))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([[1,1],[2,2]])==np.array([[1,1],[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V(np.arange(10).reshape(2,5)).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
